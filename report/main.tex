\documentclass{article}
\usepackage[]{graphicx}
%\usepackage{parskip}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{titling}
\setlength{\droptitle}{-9em} 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{cite}
\linespread{1.2}
\usepackage{hyperref}

\begin{document}
\title{CS 229 Project Proposal - Text Complexity, NLP}
\author{Harry Sha (harry2), Tyler Yep (tyep)}
\maketitle

\section{Introduction}
The goal of our project is to explore the idea of text complexity in the context of machine learning. More specifically, the questions we will answer are the following:
\begin{enumerate}
    \item What features of the text are most relevant to this classification?
    \item To what extent can machine learning methods be used to classify the complexity of a document?
    \item Can we build a model to generate or transform text into different levels of complexity?
\end{enumerate}

This project's outcomes have the potential of enhancing education immensely. Complexity-classified documents allow students to find papers or conceptual explanations at understandable difficulty level. Generating or transforming text into simpler levels of complexity encourages more widespread knowledge, approachable from different fields and backgrounds. Students gain the power to understand big picture ideas and ramp up the difficulty level as they see fit, ultimately resulting in a more personalized educational experience.

\section{Data: Feature Extraction and Selection}
We are working with the Weebit Dataset \cite{weebit}. TODO something here maybe.

\paragraph{Data preprocessing}
To preprocess the data, we removed new line characters, set all words to be lower case and removed any trademark disclaimers. We also split the dataset into training, validation and test sets. For the following section let $x$ represent one example of a preprocessed text. Word count and Tf-Idf feature extraction was completed using sci-kit learn \cite{scikit-learn}.

\paragraph{Word Count}
This creates a vocabulary $V$ of words in the training set. Then the word count feature extractor computes
\[\phi_{word count}(x)_i = \text{count of word $i$ in x} \]
We experimented with changing $min_df$, and $max_df$, which represents the minimum or maximum document frequencies of a word in order to be included in the vocabulary. Adjusting these parameters controls the dimension of $\phi_{word count}(x)$ in a logical way. Furthermore, we experimented with another parameter, $binary = True$, which sets
$$\phi_{word count}(x)_i = \begin{cases}
    1 & \text{count of word $i$ in x $\geq 1$}\\
    0 & \text{otherwise}
\end{cases}$$

Empirically, we found that performance of models were not very sensitive to the $min_df$ and $max_df$ parameters. However, the $binary$ option either substantially increased or decreased accuracy. Thus we set $min_{df}$, $max_{df} = 5, 80\%$, and tried both binary and not-binary. 

\paragraph{Tf-Idf} 
Tf-Idf extracts the word count weighted by a measure of inverse document frequency (Idf). This diminishes the importance of common words such as 'a', and 'the', and highlights the importance of uncommon words. We found that the Tf-Idf features gave worse performance the the word count feature extractor. One possible reason for this is that Tf-Idf creates feature vectors which are more similar in their topic/meaning than in structure, where for our task, the topic/meaning of the text may not be so important.

\paragraph{Natural Language Features}
We also added features for the counts of each parts of speech. Using spaCy, we parsed each of the words in our documents and categorized them by type: adjective, noun, preposition, etc. In our feature matrix, we listed the counts of each of the 17 types of prepositions for each document. We also added the average sentence length and the number of sentences to our features array. 

Finally, we can combine the features above by performing concatenation. Empirically, in logistic regression models, we found that the optimal features were a concatenation of word count with the parameters described above, and the natural language features.

\subsection{Basic Analytics on Natural Language Features} %Harry
Here we explore our data to see which of the natural language features may be promising candidates for complexity classification. Figure \ref{hists} shows the distributions of several features in each complexity level. We see that the average sentence length and document length increases with the complexity classification. Furthermore, we see that document length explains much of the variance for many of the other features as seen by comparing \ref{n}, and \ref{un}. Thus average sentence length is probably a valuable feature to use in classification.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figs/avgsentlen.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figs/length.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figs/noun.png}
        \caption{}
        \label{n}
    \end{subfigure}
    \begin{subfigure}[b]{0.23\textwidth}
        \includegraphics[width=\textwidth]{figs/unnormnoun.png}
        \caption{}
        \label{un}
    \end{subfigure}
    \caption{Unsupervised}
    \label{hists}
\end{figure}


\section{Model Selection}
TODO Have some figure/table here

\paragraph{Baseline}
Our baseline model used a Dummy classifier to randomly predict results, based on the probability of each complexity of each text appearing. In our dataset, 630/2226 examples were level 2, 789/2226 examples were level 3, and 807/2226 examples were level 4. Our baseline simply predicted each level of difficulty randomly with probabilities equal these proportions.

\paragraph{Logistic Regression} %Harry
Logistic Regression was a quite successful, with the highest accuracy on the validation set as $79.9$. The hyperparameters for Logistic Regression were the type of regularization $L_1$ or $L_2$, and the amount to regularize by, $1/C$. Lower values of $C$ mean stronger regularization. We conducted a grid search, with $C$ as the power of $10$ between $0.001$ and $100$ inclusive. We found that $L_1$ regularization generally performs better than $L_2$ regularization, this is probability $L_1$ results in sparse weights, which is advantageuous for our features, as many of them may not be useful. The effect of changing the $C$ parameter is illustrated in Figure \ref{c}. We see that the $L_1$ regularization is more sensitive the changes in the $C$.

\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.4]{figs/c.png}
    \end{center}
    \caption{Effect of $C$ on validation accuracy}
    \label{c}
\end{figure}

\paragraph{AdaBoost}
Another classifier we thought would be successful is AdaBoost. Given the relatively high results we got from using only average sentence length as a feature, we expected an ensemble of basic classifiers to perform much better.

\paragraph{Other}

\section{Next Steps}
\begin{enumerate}
    \item \textbf{Hyperparameter Tuning and Cross Validation.}To further tune the hyperparameters of our models, we plan to employ random search with k-folds validation. In particular, random search has a greater change of exploring the whole parameter space due to the randomness (TODO cite https://www.analyticsindiamag.com/why-is-random-search-better-than-grid-search-for-machine-learning/). Further the use of k-folds validation allows us to incorporate the validation set in training. Furthermore, as the score in k-folds validation is computed as an average over the performance on each fold, we are less likely to overfit to any validation set. Hopefully, using these methods, we can further improve the performance our models.
    
    \item \textbf{LSTM (sequence model).} One key drawback in our representation of the documents is that most of the sequential information is lost in the encoding. However, it is very likely that the sequential relationships play a key role in determining the complexity of a text. The reason why sequential encodings were not used originally was that models such as Logistic Regression, SVMs, expect fixed length inputs. Sequential encodings could not satisfy this requirement without padding due to the fact that different documents have different lengths. Furthermore padding did not seem like a viable option due the variance in document length. 

    To address this problem, we will use a Recurrent Neural Network. The architecture of this neural network allows for arbitrary length inputs and, can learn time dependencies. They have been successfully applied in NLP in the past. One proposed input for this model is to encode each document as a sequence of part of speech tags instead of word embeddings. Our hypothesis is that this will both allow the model to fit the data better, and also generalize to unseen texts. The intuition behind this is that there can be complex and simple text of any topic, thus a more important factor in determining complexity is grammatical structure of the sentence.

    \item \textbf{Generative Algorithm}
    Finally we will create a generative algorithm to generate texts of different levels of complexity. Our proposed method is to create $3$ LSTM models, one for each complexity level. We will use the sequence of part of speech encoding as described in the previous section, but in this setting we will be learning the probability of the part of speech of the next word given the previous words. Formally we will be predicting $p(pos^{t+1}| pos^{t}, pos^{t-1}\dots pos^{1})$. Then to generate a document, we will continually sample from the model using the previously generated samples as the input, until we reach a special terminating character. Finally we will substitute the part of speech tags with actual words from a specified dictionary. 
\end{enumerate}


\section{Data and Resources}
\begin{enumerate}
    \item Weebit (Dataset) \begin{verbatim} http://www.aclweb.org/anthology/W13-2907\end{verbatim}
    \item Cambridge English Exam (Dataset) \begin{verbatim} https://www.cl.cam.ac.uk/~mx223/cedata.html\end{verbatim}
    \item Real-Time Analysis of Reading Difficulty (Research) \begin{verbatim} https://www.cs.rochester.edu/~tetreaul/Miltsakaki.pdf\end{verbatim}
    \item Automatic Text Difficulty Classifier for European Portuguese Teaching (Research) \begin{verbatim} http://www.inesc-id.pt/publications/11043/pdf\end{verbatim}
\end{enumerate}

\bibliographystyle{plain}
\bibliography{biblio}

%\section{Plan}
%\subsection{Classification of Complexity}
%We will then experiment with many types of classification algorithms such as SVMs, linear models, k-nearest neighbours, decision trees, and deep learning models such as LSTMs. The different approaches can then be compared/evaluated and potentially used in ensemble, and then will be evaluated using cross-validation. 
%
%Complexity in past papers have been defined in different ways, from Lexile scores to age ranges, which we can experiment with. Ultimately, the complexity measurement we choose will stem from the datasets we analyze, however we do also have the option of reclassifying complexity data into broader categories.
%
%We also intend to explore non-traditional way of classifying complexity, perhaps inspired by the popular WIRED video series "5 Levels of Difficulty". Here, complexity is classified into child, teenager, adult, professional, and expert, which are very distinct categories. These distinct categories will cause the important distinctions to surface within our model, which we can then leverage in our final step, which is generating text at varying difficulties.
%
%\subsection{Generation of Text}
%Finally, we will create a model that generates or transforms text to different difficulties, or recreating the same text in a different complexity category. In particular, we are interested in simplifying complex texts - finding ways to express the same explanation of a concept, but understandable to a more general audience. Some initial ideas for implementations are to use LSTMs or GANs, which have seen success in generating sequences and images respectively. 
%
%One major application of this research goal is in academia - specifically, in the increasing readability difficulty of research papers. One reach goal of this project would be to find ways to safely and effectively simplify jargon into more colloquial forms, without sacrificing the overall meaning. To evaluate this model, we will find or create a rubric to distinguish the different complexity classes. Then, we will manually compare the generated output to our established rubric to measure the success of our model.
%
%By making research more accessible to the public, we would invite more collaborators into complex fields, ultimately accelerating the growth rate of human knowledge.
\end{document}
