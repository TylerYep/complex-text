\documentclass{article}
\usepackage[]{graphicx}
%\usepackage{parskip}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{titling}
\setlength{\droptitle}{-9em} 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{cite}
\linespread{1}
\usepackage{hyperref}
\setlength\parindent{0pt}

\begin{document}
\title{CS 229 Project Milestone: Text Complexity (NLP)}
\author{Harry Sha (harry2), Tyler Yep (tyep)}
\maketitle

\section{Introduction}
The goal of our project is to explore text complexity in the context of machine learning. More specifically, we will answer the following questions:
\begin{enumerate}
    \item What features of the text are most relevant to this classification?
    \item To what extent can machine learning methods be used to classify the complexity of a document?
    \item Can we build a model to generate or transform text into different levels of complexity?
\end{enumerate}

This project's outcomes have the potential of enhancing education immensely. Complexity-classified documents allow students to find papers or conceptual explanations at understandable difficulty level. Generating or transforming text into simpler levels of complexity encourages more widespread knowledge, approachable from different fields and backgrounds. Students gain the power to understand big picture ideas and ramp up the difficulty level as they see fit, ultimately resulting in a more personalized educational experience.

\section{Data: Feature Extraction and Selection}
We are using the Weebit Dataset \cite{weebit}, which has 2226 examples separated into 3 different reading levels.

\paragraph{Data Preprocessing}
To preprocess the data, we removed new line characters, set all words to be lower case and removed any trademark disclaimers. We also split the dataset into training, validation and test sets. For the following section, let $x$ represent one example of a preprocessed text. Word count and Tf-Idf feature extraction were completed using sci-kit learn \cite{scikit-learn}.

\paragraph{Word Count}
First, we created a vocabulary $V$ of words in the training set. Our word count feature extractor computes:
\[\phi_{word count}(x)_i = \text{count of word $i$ in x} \]
We experimented with changing $min_{df}$, and $max_{df}$, which represents the minimum or maximum document frequencies of a word in order to be included in the vocabulary. Adjusting these parameters controled the dimension of $\phi_{word count}(x)$. Furthermore, we experimented with another parameter, $binary = True$, which changed our feature extractor to:
$$\phi_{word count}(x)_i = \begin{cases}
    1 & \text{count of word $i$ in x $\geq 1$}\\
    0 & \text{otherwise}
\end{cases}$$

Empirically, we found that performance of models were not very sensitive to the $min_{df}$ and $max_{df}$ parameters. However, the $binary = True$ option substantially increased accuracy. In our analysis, we set $min_{df} = 5$, $max_{df} = 80\%$, and tried both binary and non-binary word counting. 

\paragraph{Tf-Idf} 
Tf-Idf extracts the word count weighted by a measure of inverse document frequency (Idf). This diminishes the importance of common words such as 'a', and 'the', and highlights the importance of uncommon words. However, we found that the Tf-Idf features gave worse performance than the word count feature extractor. One possible reason for this is that Tf-Idf creates feature vectors which are more similar in their topic/meaning than in their structure. In our task, the topic/meaning of the text may not be as important as the ordering and structure of the words.

\paragraph{Natural Language Features}
We also added features for the counts of each parts of speech. Using spaCy \cite{spacy2}, we parsed each of the words in our documents and categorized them by type: adjective, noun, preposition, etc. In our feature matrix, we listed the counts of each of the 17 types of prepositions for each document. We also added the average sentence length and the number of sentences to our features array. Furthermore, we combined some of the aforementioned features through concatenation. Empirically, in models like logistic regression, we found that the optimal features were a concatenation of word count with the parameters described above, and the natural language features.

\subsection{Basic Analytics on Natural Language Features} %Harry
Here, we explored our data to see which of the natural language features were the most promising candidates for complexity classification. Figure \ref{hists} shows the distributions of several features in each complexity level. We see that the average sentence length and document length increased with the complexity classification. Furthermore, we see that document length explains much of the variance for many of the other features as seen by comparing \ref{n}, and \ref{un}. Though average sentence length is a valuable feature to use in classification, it is by no means a perfect indicator, as shown by the large overlapping region between levels in Figure 1a.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{figs/avgsentlen.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{figs/length.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/noun.png}
        \caption{}
        \label{n}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/unnormnoun.png}
        \caption{}
        \label{un}
    \end{subfigure}
    \caption{Natural Language Features}
    \label{hists}
\end{figure}


\section{Model Selection}
\paragraph{Baseline}
Our baseline model used a Dummy classifier to randomly predict results, based on the probability of each complexity of each text appearing. In our dataset, 630/2226 examples were level 2, 789/2226 examples were level 3, and 807/2226 examples were level 4. Our baseline simply predicted each level of difficulty randomly with probabilities equal these proportions, and obtained $37.2\%$ accuracy on the validation set.

\paragraph{Logistic Regression}
Logistic Regression was very successful, with the highest accuracy on the validation set at $79.9\%$. The hyperparameters for Logistic Regression were type of regularization ($L_1$ or $L_2$), and the amount to regularize by, $1/C$. We conducted a grid search, trying $C$ as powers of $10$ between $0.001$ and $100$ inclusive. We found that $L_1$ regularization generally performed better than $L_2$ regularization. This is likely because probability $L_1$ results in sparse weights, which was advantageous in identifying and reducing the effects of less-useful features. The effect of changing the $C$ parameter is illustrated in Figure \ref{c}. We see that the $L_1$ regularization is more sensitive the changes in $C$.

\begin{figure}[h]
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/c.png}
        \caption{Effect of $C$ on validation accuracy}
        \label{c}
    \end{subfigure}
    \begin{subfigure}[d]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figs/e.png}
        \caption{AdaBoost w/ different parameters}
        \label{d}
    \end{subfigure}
    \caption{Parameter Tuning}
\end{figure}

\paragraph{AdaBoost}
Another successful classifier was AdaBoost. Given the relatively high results we obtained from using only average sentence length as a feature, we expected an ensemble of basic classifiers to perform much better. After tuning the number of classifiers and the learning rate, AdaBoost achieved $79.7\%$ on the validation set. In Figure \ref{d}, we have a plot of AdaBoost test accuracy using different learning rates and estimators (using only word count + natural language features).

\paragraph{Other}
Other algorithms we tried, such as Naive Bayes or k-Nearest-Neighbors, performed better than our baseline, but did not have as much initial success as AdaBoost and Logistic Regression, and did not seem to fit our problem as well. For example, they made questionable assumptions of independence or modeled complex documents as clusters, which did not fit with our selected features.

\section{Recurrent Neural Networks}
In this section, we explore the usage of Recurrent Neural Networks, in particular LSTMs for the complexity classification task. One key drawback in our representation of the documents so far is that all of the sequential information is lost in the feature encoding. For example any permutation of the document results in the same feature vector. However, sequential relationships likely play a key role in determining the complexity of a text. We did not use sequential encodings originally because models such as Logistic Regression, expect fixed length inputs. Because our documents were of different lengths, sequential encodings could not satisfy this requirement without padding, and padding did not seem like a viable option due the variance in document length. 

To address this problem, we can use a Recurrent Neural Network. The architecture of this neural network allows for arbitrary length inputs and can even learn time dependencies. They have also been successfully applied in NLP in the past. One viable input for this model is to encode each document as a sequence of part of speech tags instead of word embeddings. Our hypothesis is that this will both allow the model to fit the data better, and also generalize to unseen texts. The intuition behind this is that there can be complex and simple text of any topic, and thus a more important factor in determining complexity is the grammatical structure of the sentence. We hypothesize that using word embeddings will cause the model to overfit. 

\subsection{Model Architecture}
\begin{enumerate}
    \item \textbf{Encoding.} As mentioned, the encoding we chose was to represent texts as a sequence of part of speech tags, However, we chose to replace the \textit{PUNCT} tag with the actual punctuation used in the sentence. We did this as the exact punctuation used is probably useful in determining the complexity of a sentence. For example, if we just used \textit{PUNCT}, commas and periods would be indistinguishable, and the algorithm may not know the difference between a series of simple sentences and compound sentence. The final vocabulary consists of $46$ part of speech tags and and punctuation marks. For the rest of the report let $V$ be the vocabulary where $V_i$ is the $i^{th}$ item in the vocabulary. 
    \item \textbf{Embedding Layer} The first layer of the LSTM is an embedding layer. This is inspired by NLP methods which typically use word2vec or trainable embedding layers to represent each item of the vocabulary. For our case, the embedding layer takes in a element of the vocabulary and maps it to a $EMBED\_DIM$ dimensional vector, which is trained using the optimization algorithm. 
    \item \textbf{LSTM}
        The embeddings are then put into a LSTM model. The tunable parameters of this step are
        \begin{itemize}
            \item \textit{N\_LAYERS}, the number of LSTM layers.
            \item \textit{HIDDEN\_DIM}, the dimension of each LSTM layer.
            \item \textit{DROPOUT}, the percentage of neurons that are deactivated in each LSTM layer.
        \end{itemize}
        The LSTM has an output for each value in the sequence. Thus to get a fixed length vector for input to the next linear layer, we experimented with either using the output at the final value in the sequence, and using the mean output across all values in the sequence. 
    \item \textbf{Linear}
        The output of the LSTM layer is then fed into a linear layer $Wx + b$. The purpose of this layer is mainly to transform the dimension of output of the LSTM layer to \textit{HIDDEN\_DIM} to $3$ as we are trying to predict 3 levels of difficulty.
    \item \textbf{Softmax}
        Finally, the outputs of the linear layer are fed into a softmax layer, which normalizes the outputs so that they can be interpreted as the probabilities that the text was of each of the levels. 
\end{enumerate}

\subsection{Results}
% Used Adam algorithm to train
%TODO tyler: 
% Hyperparameter tuning
% Some pretty loss plots and critical comparison with previous models



TODO include specifics about the implementation?

\section{Text Generation}
The final goal of this project was to experiment with generating texts of different complexity levels. In this project we focus on grammar and sentence/document structure as the primary determinant of complexity. Therefore we employ LSTM models again, but this time for sequence prediction instead of classification. In particular given the sequence of part of speech encodings described in the previous section, the model learns $p(pos^{t+1} \mid pos^{t}, pos^{t-1},\dots, pos^{1})$. We can sample from this probability distribution to generate sequences of part of speech tags.

In the encoding we introduce two special characters '$<$', and '$>$' which are used to signify the beginning and the end of a text. These are used so that the model learns when to end a text, which is useful for sampling. Apart from these changes, the model architecure is equivalent to that in the previous section.

\subsection{Training}

\subsection{Sampling}
Let $G$ be the trained model, $V$ be the vocabulary, $D = |V|$, and $x$ be a document. It is an approximation such that \[G(x_1,x_2,\dots,x_{t-1}) \approx \begin{bmatrix}
p(x_t = V_1| \{x_j\mid j < t\}) \\
p(x_t = V_2| \{x_j\mid j < t\}) \\
\vdots\\
p(x_t = V_D| \{x_j\mid j < t\})
\end{bmatrix}\]
Where $x_t$ is the $t^{th}$ part of speech tag in the document $x$. To sample a sequence of part of speech tags we used the following algorithm:
\begin{verbatim}
def sample(temp):
    seq = []
    x_t = `<'
    while True:
        probabilities = G(seq)
        probs = normalize([p**(1/temp) for p in prob])
        next = choose(prob)
        seq.append(V[next])
        if next == `>': return seq
\end{verbatim}
Here the parameter \textit{temp}, inspired by language and music generation models, controls the rigidness of the sampling. In particular, low values for temperature result in more grammatically correct sentences, and higher values encourage more diverse sentence structures. \textit{normalize} is a function that normalizes a vector so that it sums to 1, and \textit{choose(prob)} is a function that samples from a multinomial distribution with parameter \textit{prob}. 

TODO substitution part (optional?). 

\section{Learning}
TODO training results and some example sentences from each difficulty level.


\section{Conclusion}
TODO


\begin{enumerate}
    \item \textbf{Hyperparameter Tuning \& Cross Validation} To further tune the hyperparameters of our models, we plan to employ random search with k-fold validation. Random search has a greater chance of exploring the whole parameter space \cite{random}, and will allow us to incorporate the validation set in training. Furthermore, since the score in k-fold validation is computed as an average over the performance on each fold, we are less likely to overfit to any validation set. Hopefully, using these methods, we will further improve the performance of our models.
\end{enumerate}

\section{Contributions, Code}
Harry - Word count, Tf-Idf feature extraction, exploration of natural language features, experiments with Logistic Regression, SVM, Naive Bayes, organizational code.\\
Tyler - Natural language feature extraction, AdaBoost, Naive Bayes, baseline results \\
Code can be found at: https://github.com/TylerYep/complex-text. 

%\section{Data and Resources}
%\begin{enumerate}
%    \item Weebit (Dataset) \begin{verbatim} http://www.aclweb.org/anthology/W13-2907\end{verbatim}
%    \item Cambridge English Exam (Dataset) \begin{verbatim} https://www.cl.cam.ac.uk/~mx223/cedata.html\end{verbatim}
%    \item Real-Time Analysis of Reading Difficulty (Research) \begin{verbatim} https://www.cs.rochester.edu/~tetreaul/Miltsakaki.pdf\end{verbatim}
%    \item Automatic Text Difficulty Classifier for European Portuguese Teaching (Research) \begin{verbatim} http://www.inesc-id.pt/publications/11043/pdf\end{verbatim}
%\end{enumerate}

\bibliographystyle{plain}
\bibliography{biblio}

%\section{Plan}
%\subsection{Classification of Complexity}
%We will then experiment with many types of classification algorithms such as SVMs, linear models, k-nearest neighbours, decision trees, and deep learning models such as LSTMs. The different approaches can then be compared/evaluated and potentially used in ensemble, and then will be evaluated using cross-validation. 
%
%Complexity in past papers have been defined in different ways, from Lexile scores to age ranges, which we can experiment with. Ultimately, the complexity measurement we choose will stem from the datasets we analyze, however we do also have the option of reclassifying complexity data into broader categories.
%
%We also intend to explore non-traditional way of classifying complexity, perhaps inspired by the popular WIRED video series "5 Levels of Difficulty". Here, complexity is classified into child, teenager, adult, professional, and expert, which are very distinct categories. These distinct categories will cause the important distinctions to surface within our model, which we can then leverage in our final step, which is generating text at varying difficulties.
%
%\subsection{Generation of Text}
%Finally, we will create a model that generates or transforms text to different difficulties, or recreating the same text in a different complexity category. In particular, we are interested in simplifying complex texts - finding ways to express the same explanation of a concept, but understandable to a more general audience. Some initial ideas for implementations are to use LSTMs or GANs, which have seen success in generating sequences and images respectively. 
%
%One major application of this research goal is in academia - specifically, in the increasing readability difficulty of research papers. One reach goal of this project would be to find ways to safely and effectively simplify jargon into more colloquial forms, without sacrificing the overall meaning. To evaluate this model, we will find or create a rubric to distinguish the different complexity classes. Then, we will manually compare the generated output to our established rubric to measure the success of our model.
%
%By making research more accessible to the public, we would invite more collaborators into complex fields, ultimately accelerating the growth rate of human knowledge.
\end{document}
