\documentclass{article}
\usepackage[]{graphicx}
%\usepackage{parskip}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{titling}
\setlength{\droptitle}{-9em} 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{cite}
\linespread{1}
\usepackage{hyperref}
\setlength\parindent{0pt}

\begin{document}
\title{CS 229 Project Proposal - Text Complexity, NLP}
\author{Harry Sha (harry2), Tyler Yep (tyep)}
\maketitle

\section{Introduction}
The goal of our project is to explore text complexity in the context of machine learning. More specifically, we will answer the following questions:
\begin{enumerate}
    \item What features of the text are most relevant to this classification?
    \item To what extent can machine learning methods be used to classify the complexity of a document?
    \item Can we build a model to generate or transform text into different levels of complexity?
\end{enumerate}

This project's outcomes have the potential of enhancing education immensely. Complexity-classified documents allow students to find papers or conceptual explanations at understandable difficulty level. Generating or transforming text into simpler levels of complexity encourages more widespread knowledge, approachable from different fields and backgrounds. Students gain the power to understand big picture ideas and ramp up the difficulty level as they see fit, ultimately resulting in a more personalized educational experience.

\section{Data: Feature Extraction and Selection}
We are using the Weebit Dataset \cite{weebit}, which has 2226 examples separated into 3 different reading levels.

\paragraph{Data Preprocessing}
To preprocess the data, we removed new line characters, set all words to be lower case and removed any trademark disclaimers. We also split the dataset into training, validation and test sets. For the following section, let $x$ represent one example of a preprocessed text. Word count and Tf-Idf feature extraction were completed using sci-kit learn \cite{scikit-learn}.

\paragraph{Word Count}
First, we created a vocabulary $V$ of words in the training set. Our word count feature extractor computes:
\[\phi_{word count}(x)_i = \text{count of word $i$ in x} \]
We experimented with changing $min_{df}$, and $max_{df}$, which represents the minimum or maximum document frequencies of a word in order to be included in the vocabulary. Adjusting these parameters controled the dimension of $\phi_{word count}(x)$. Furthermore, we experimented with another parameter, $binary = True$, which changed our feature extractor to:
$$\phi_{word count}(x)_i = \begin{cases}
    1 & \text{count of word $i$ in x $\geq 1$}\\
    0 & \text{otherwise}
\end{cases}$$

Empirically, we found that performance of models were not very sensitive to the $min_{df}$ and $max_{df}$ parameters. However, the $binary = True$ option substantially increased accuracy. In our analysis, we set $min_{df} = 5$, $max_{df} = 80\%$, and tried both binary and non-binary word counting. 

\paragraph{Tf-Idf} 
Tf-Idf extracts the word count weighted by a measure of inverse document frequency (Idf). This diminishes the importance of common words such as 'a', and 'the', and highlights the importance of uncommon words. However, we found that the Tf-Idf features gave worse performance than the word count feature extractor. One possible reason for this is that Tf-Idf creates feature vectors which are more similar in their topic/meaning than in their structure. In our task, the topic/meaning of the text may not be as important as the ordering and structure of the words.

\paragraph{Natural Language Features}
We also added features for the counts of each parts of speech. Using spaCy \cite{spacy2}, we parsed each of the words in our documents and categorized them by type: adjective, noun, preposition, etc. In our feature matrix, we listed the counts of each of the 17 types of prepositions for each document. We also added the average sentence length and the number of sentences to our features array. Furthermore, we combined some of the aforementioned features through concatenation. Empirically, in models like logistic regression, we found that the optimal features were a concatenation of word count with the parameters described above, and the natural language features.

\subsection{Basic Analytics on Natural Language Features} %Harry
Here, we explored our data to see which of the natural language features were the most promising candidates for complexity classification. Figure \ref{hists} shows the distributions of several features in each complexity level. We see that the average sentence length and document length increased with the complexity classification. Furthermore, we see that document length explains much of the variance for many of the other features as seen by comparing \ref{n}, and \ref{un}. Though average sentence length is a valuable feature to use in classification, it is by no means a perfect indicator, as shown by the large overlapping region between levels in Figure 1a.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{figs/avgsentlen.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.25\textwidth}
        \includegraphics[width=\textwidth]{figs/length.png}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/noun.png}
        \caption{}
        \label{n}
    \end{subfigure}
    \begin{subfigure}[b]{0.24\textwidth}
        \includegraphics[width=\textwidth]{figs/unnormnoun.png}
        \caption{}
        \label{un}
    \end{subfigure}
    \caption{Natural Language Features}
    \label{hists}
\end{figure}


\section{Model Selection}
\paragraph{Baseline}
Our baseline model used a Dummy classifier to randomly predict results, based on the probability of each complexity of each text appearing. In our dataset, 630/2226 examples were level 2, 789/2226 examples were level 3, and 807/2226 examples were level 4. Our baseline simply predicted each level of difficulty randomly with probabilities equal these proportions, and obtained $37.2\%$ accuracy on the validation set.

\paragraph{Logistic Regression}
Logistic Regression was very successful, with the highest accuracy on the validation set at $79.9\%$. The hyperparameters for Logistic Regression were type of regularization ($L_1$ or $L_2$), and the amount to regularize by, $1/C$. We conducted a grid search, trying $C$ as powers of $10$ between $0.001$ and $100$ inclusive. We found that $L_1$ regularization generally performed better than $L_2$ regularization. This is likely because probability $L_1$ results in sparse weights, which was advantageous in identifying and reducing the effects of less-useful features. The effect of changing the $C$ parameter is illustrated in Figure \ref{c}. We see that the $L_1$ regularization is more sensitive the changes in $C$.

\begin{figure}[h]
    \centering
    \begin{subfigure}[c]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figs/c.png}
        \caption{Effect of $C$ on validation accuracy}
        \label{c}
    \end{subfigure}
    \begin{subfigure}[d]{0.4\textwidth}
        \includegraphics[width=\textwidth]{figs/e.png}
        \caption{AdaBoost w/ different parameters}
        \label{d}
    \end{subfigure}
    \caption{Parameter Tuning}
\end{figure}

\paragraph{AdaBoost}
Another successful classifier was AdaBoost. Given the relatively high results we obtained from using only average sentence length as a feature, we expected an ensemble of basic classifiers to perform much better. After tuning the number of classifiers and the learning rate, AdaBoost achieved $79.7\%$ on the validation set. In Figure \ref{d}, we have a plot of AdaBoost test accuracy using different learning rates and estimators (using only word count + natural language features).

\paragraph{Other}
Other algorithms we tried, such as Naive Bayes or k-Nearest-Neighbors, performed better than our baseline, but did not have as much initial success as AdaBoost and Logistic Regression, and did not seem to fit our problem as well. For example, they made questionable assumptions of independence or modeled complex documents as clusters, which did not fit with our selected features.

\section{Next Steps}
\begin{enumerate}
    \item \textbf{Hyperparameter Tuning \& Cross Validation} To further tune the hyperparameters of our models, we plan to employ random search with k-fold validation. Random search has a greater chance of exploring the whole parameter space \cite{random}, and will allow us to incorporate the validation set in training. Furthermore, since the score in k-fold validation is computed as an average over the performance on each fold, we are less likely to overfit to any validation set. Hopefully, using these methods, we will further improve the performance of our models.
    
    \item \textbf{LSTM (Sequence Model)} One key drawback in our representation of the documents is that most of the sequential information is lost in the feature encoding. However, sequential relationships likely play a key role in determining the complexity of a text. We did not use sequential encodings originally because models such as Logistic Regression, expect fixed length inputs. Because our documents were of different lengths, sequential encodings could not satisfy this requirement without padding, and padding did not seem like a viable option due the variance in document length. 

    To address this problem, we can use a Recurrent Neural Network. The architecture of this neural network allows for arbitrary length inputs and can even learn time dependencies. They have also been successfully applied in NLP in the past. One viable input for this model is to encode each document as a sequence of part of speech tags instead of word embeddings. Our hypothesis is that this will both allow the model to fit the data better, and also generalize to unseen texts. The intuition behind this is that there can be complex and simple text of any topic, and thus a more important factor in determining complexity is the grammatical structure of the sentence.

    \item \textbf{Generative Algorithm}
    Finally, we will create a generative algorithm to generate texts of different levels of complexity. Our proposed method is to create $3$ LSTM models, one for each complexity level. We will use the sequence of part of speech encoding as described in the previous section, but in this setting we will be learning the probability of the part of speech of the next word given the previous words. Formally, we will be predicting $P(pos^{t+1} \mid pos^{t}, pos^{t-1},\dots, pos^{1})$. Then, to generate a document, we will continually sample from the model using the previously generated samples as the input, until we reach a special terminating character. Finally, we will substitute the part of speech tags with actual words from a specified dictionary. 
\end{enumerate}

\section{Contributions, Code}
Harry - Word count, Tf-Idf feature extraction, exploration of natural language features, experiments with Logistic Regression, SVM, Naive Bayes, organizational code.\\
Tyler - Natural language feature extraction, AdaBoost, Naive Bayes, baseline results \\
Code can be found at: https://github.com/TylerYep/complex-text. 

%\section{Data and Resources}
%\begin{enumerate}
%    \item Weebit (Dataset) \begin{verbatim} http://www.aclweb.org/anthology/W13-2907\end{verbatim}
%    \item Cambridge English Exam (Dataset) \begin{verbatim} https://www.cl.cam.ac.uk/~mx223/cedata.html\end{verbatim}
%    \item Real-Time Analysis of Reading Difficulty (Research) \begin{verbatim} https://www.cs.rochester.edu/~tetreaul/Miltsakaki.pdf\end{verbatim}
%    \item Automatic Text Difficulty Classifier for European Portuguese Teaching (Research) \begin{verbatim} http://www.inesc-id.pt/publications/11043/pdf\end{verbatim}
%\end{enumerate}

\bibliographystyle{plain}
\bibliography{biblio}

%\section{Plan}
%\subsection{Classification of Complexity}
%We will then experiment with many types of classification algorithms such as SVMs, linear models, k-nearest neighbours, decision trees, and deep learning models such as LSTMs. The different approaches can then be compared/evaluated and potentially used in ensemble, and then will be evaluated using cross-validation. 
%
%Complexity in past papers have been defined in different ways, from Lexile scores to age ranges, which we can experiment with. Ultimately, the complexity measurement we choose will stem from the datasets we analyze, however we do also have the option of reclassifying complexity data into broader categories.
%
%We also intend to explore non-traditional way of classifying complexity, perhaps inspired by the popular WIRED video series "5 Levels of Difficulty". Here, complexity is classified into child, teenager, adult, professional, and expert, which are very distinct categories. These distinct categories will cause the important distinctions to surface within our model, which we can then leverage in our final step, which is generating text at varying difficulties.
%
%\subsection{Generation of Text}
%Finally, we will create a model that generates or transforms text to different difficulties, or recreating the same text in a different complexity category. In particular, we are interested in simplifying complex texts - finding ways to express the same explanation of a concept, but understandable to a more general audience. Some initial ideas for implementations are to use LSTMs or GANs, which have seen success in generating sequences and images respectively. 
%
%One major application of this research goal is in academia - specifically, in the increasing readability difficulty of research papers. One reach goal of this project would be to find ways to safely and effectively simplify jargon into more colloquial forms, without sacrificing the overall meaning. To evaluate this model, we will find or create a rubric to distinguish the different complexity classes. Then, we will manually compare the generated output to our established rubric to measure the success of our model.
%
%By making research more accessible to the public, we would invite more collaborators into complex fields, ultimately accelerating the growth rate of human knowledge.
\end{document}
